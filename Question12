
## Load in and run tokenizer on all four gold datasets - train, dev, devtest, and test 

dev = pd.read_csv("Gold/dev.txt", sep='\t', header=None, 
                   names=['id', 'class', 'tweet'], encoding='utf-8')
dev.drop(['id'], axis=1, inplace=True)

train = pd.read_csv("Gold/train.txt", sep='\t', header=None, 
                   names=['id', 'class', 'tweet'], encoding='utf-8')
train.drop(['id'], axis=1, inplace=True)

devtest = pd.read_csv("Gold/devtest.txt", sep='\t', header=None, 
                   names=['id', 'class', 'tweet'], encoding='utf-8')
devtest.drop(['id'], axis=1, inplace=True)
devtest.loc[1,'tweet'] = "@PersonaSoda well yeah, that's third parties. Sony itself isn't putting out actual games for it. It's got 1-2 yrs of 3rd party support left."
devtest.loc[1,'class'] = 'neutral'

test = pd.read_csv("test.txt", sep='\t', header=None, index_col=False,
                   names=['id', 'class', 'tweet'], encoding='utf-8')
test.drop(['id'], axis=1, inplace=True)


# Define list that will hold all token types for Gold dev data
temp_gold_dev = []

for text in dev['tweet'].values:
    # Each time the REGEX matches, add the matched string to the running list of all words
    for matches in my_extensible_tokenizer.findall(text):
         #just in case get empty matches
        if matches != '':
            # Add string to master list
            temp_gold_dev.append(matches)
# Save number of tokens in dev set
full1 = len(temp_gold_dev)            

# Define list that will hold all token types for Gold training data
temp_gold_train = []

for text in train['tweet'].values:
    # Each time the REGEX matches, add the matched string to the running list of all words
    for matches in my_extensible_tokenizer.findall(text):
         #just in case get empty matches
        if matches != '':
            # Add string to master list
            temp_gold_train.append(matches)
# Save number of tokens in training set        
full2 = len(temp_gold_train)            
            
# Define list that will hold all token types for Gold DEVTEST data
temp_gold_devtest = []

for text in devtest['tweet'].values:
    # Each time the REGEX matches, add the matched string to the running list of all words
    for matches in my_extensible_tokenizer.findall(text):
         #just in case get empty matches
        if matches != '':
            # Add string to master list
            temp_gold_devtest.append(matches)
# Save number of tokens in devset   
full3 = len(temp_gold_devtest)        

# Define list that will hold all token types for Gold test data
temp_gold_test = []

for text in test['tweet'].values:
    # Each time the REGEX matches, add the matched string to the running list of all words
    for matches in my_extensible_tokenizer.findall(text):
         #just in case get empty matches
        if matches != '':
            # Add string to master list
            temp_gold_test.append(matches)

full4 = len(temp_gold_test)

# Create null dictionaries that will store types and counts for each dataset
corpus_gold_dev = {}
corpus_gold_train = {}
corpus_gold_devtest = {}
corpus_gold_test = {}

# Create null dictionary that will store types and counts for all gold datasets combined
corpus_gold = {}

# Looking through entire list of (repeated) words, count instances of distinct words
for word in temp_gold_dev:
    # If word has already been seen, add one to its count
    if word in corpus_gold_dev:
        corpus_gold_dev[word] += 1
    # If word has not already been seen, add word to list
    else:
        corpus_gold_dev[word] = 1
    if word in corpus_gold:
        corpus_gold[word] += 1
    else:
        corpus_gold[word] = 1
# Number of unique words in dev set
unique1 = len(corpus_gold)



# Looking through entire list of (repeated) words, count instances of distinct words
for word in temp_gold_train:
    # If word has already been seen, add one to its count
    if word in corpus_gold_train:
        corpus_gold_train[word] += 1
    # If word has not already been seen, add word to list
    else:
        corpus_gold_train[word] = 1
        
    if word in corpus_gold:
        corpus_gold[word] += 1
    else:
        corpus_gold[word] = 1
# Number of unique words in training set
unique2 = len(corpus_gold)


# Looking through entire list of (repeated) words, count instances of distinct words
for word in temp_gold_devtest:
    # If word has already been seen, add one to its count
    if word in corpus_gold_devtest:
        corpus_gold_devtest[word] += 1
    # If word has not already been seen, add word to list
    else:
        corpus_gold_devtest[word] = 1

    if word in corpus_gold:
        corpus_gold[word] += 1
    else:
        corpus_gold[word] = 1       
# Number of unique words in devtest set
unique3 = len(corpus_gold)


# Looking through entire list of (repeated) words, count instances of distinct words
for word in temp_gold_test:
    # If word has already been seen, add one to its count
    if word in corpus_gold_test:
        corpus_gold_test[word] += 1
    # If word has not already been seen, add word to list
    else:
        corpus_gold_test[word] = 1

    if word in corpus_gold:
        corpus_gold[word] += 1
    else:
        corpus_gold[word] = 1     
# Number of unique words in test set
unique4 = len(corpus_gold)



# 11. Number of types that appear in dev data but not training data
# 5062
    
# Define variable that will store values found in exclusively the dev data
justdev = corpus_gold_dev.keys() - corpus_gold_train.keys()

print('Number of types found in dev data but not in training data: ' + str(len(justdev)))



# 12. Compare vocab size of combined gold datasets versus input dataset.
# Plot vocab growth at different sizes N?????????
# 41760 in input dataset, 24783 in combined gold dataset.
print("Distinct words in input dataset: " + str(len(corpus)) + "\nDistinct words in gold dataset: " + str(len(corpus_gold)))

# Use counts recorded above to store the total number of tokens and unique words as each of the four subsets of the Gold dataset are incorporated into the lexicon
lexicon = pd.DataFrame({'unique':[0,unique1,unique2,unique3,unique4],'total':[0,full1,full1+full2,full1+full2+full3,full1+full2+full3+full4]})
lexicon.plot(x ='total', y='unique', kind = 'line', title='Gold Dataset',marker='.', markersize=15,legend=None)
plt.xlabel("Total Number of Words")
plt.ylabel("Unique Words in Lexicon")
