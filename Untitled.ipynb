{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import pandas as pd \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from statistics import stdev\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv(\"Dev/INPUT.txt\", sep='\\t', header=None, \n",
    "                   names=['id', 'target', 'tweet'], encoding='utf-8')\n",
    "data.drop(['id', 'target'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  Input Data\n",
    "Total number of tweets is 11906.  \n",
    "Total number of characters is 1354375.  \n",
    "Total number of distinct words (vocabulary) is 41760.  \n",
    "Average number of words and characters per tweet is 16.066 and 113.756, respectively.  \n",
    "Average number and standard deviation of characters and token 5.979 and 4.706. \n",
    "Total tokens corresponding to 10 most frequent words is 23268.  \n",
    "Number of distinct word n-grams for n=2,3,4,5:\n",
    "- n-2: 130116\n",
    "- n-3: 157675\n",
    "- n-4: 153193\n",
    "- n-5: 142858\n",
    "\n",
    "Number of distinct character n-grams for n=2,3,4,5,6,7:\n",
    "- n-2: 8069\n",
    "- n-3: 98192\n",
    "- n-4: 212264\n",
    "- n-5: 361987\n",
    "- n-6: 520197\n",
    "- n-7: 662468  \n",
    "\n",
    "Number of word types that appear in dev data but not training data is 5062.  \n",
    "Token type ratio is 4.58.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of tweets: \" + str(data.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 0\n",
    "for tweet in data.values:\n",
    "    # Add to total character count\n",
    "    num += len(tweet[0])\n",
    "print(\"Total number of characters, including spaces: \" + str(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexes=(\n",
    "# Keep usernames together (any token starting with @, followed by A-Z, a-z, 0-9)        \n",
    "r\"(?:@[\\w_]+)\",\n",
    "            \n",
    "# Keep hashtags together (any token starting with #, followed by A-Z, a-z, 0-9, _, or -)\n",
    "r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\",\n",
    "\n",
    "# abbreviations, e.g. U.S.A.\n",
    "r'(?:[A-Z]\\.)+',\n",
    "r'[A-Za-z]\\.(?:[A-Za-z0-9]\\.)+',\n",
    "r'[A-Z][bcdfghj-np-tvxz]+\\.',\n",
    "\n",
    "# URL, e.g. https://google.com\n",
    "r'https?:\\/\\/(?:www\\.',\n",
    "r'(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}',\n",
    "r'www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}',\n",
    "r'https?:\\/\\/(?:www\\.',\n",
    "r'(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}',\n",
    "r'www\\.[a-zA-Z0-9]+\\.[^\\s]{2,}',\n",
    "\n",
    "# currency and percentages, e.g. $12.40, 82%\n",
    "r'\\$?\\d+(?:\\.\\d+)?%?',\n",
    "\n",
    "# Numbers i.e. 123,56.34\n",
    "r'(?:[0-9]+[,]?)+(?:[.][0-9]+)?',\n",
    "\n",
    "# Keep words with apostrophes, hyphens, and underscores together\n",
    "r\"(?:[a-z][a-zâ€™'\\-_]+[a-z])\",\n",
    "\n",
    "# Keep all other sequences of A-Z, a-z, 0-9, _ together\n",
    "r\"(?:[\\w_]+)\",\n",
    "\n",
    "# Match words at the end of a sentence.  e.g. tree. or tree!\n",
    "r'(?:[a-z]+(?=[.!\\?]))',\n",
    "\n",
    "# Everything else that's not whitespace\n",
    "# It seems like this captures punctuations and emojis and emoticons.  \n",
    "#r\"(?:\\S)\"\n",
    ")\n",
    "big_regex=\"|\".join(regexes)\n",
    "\n",
    "# Note re.I for performing Perform case-insensitive matching; \n",
    "# expressions like [A-Z] will match lowercase letters, too. \n",
    "my_extensible_tokenizer = re.compile(big_regex, re.VERBOSE | re.I | re.UNICODE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list that will hold all token\n",
    "temp = []\n",
    "\n",
    "# Define dictionaries that will store word 2-, 3-, 4-, and 5- grams\n",
    "corp_2gram = {}\n",
    "corp_3gram = {}\n",
    "corp_4gram = {}\n",
    "corp_5gram = {}\n",
    "\n",
    "# Iterate through each tweet\n",
    "for text in data.values:\n",
    "    # Define counter that will identify the index for the word in the current tweet\n",
    "    count2 = 0\n",
    "    # Define string that will store current matches in a given tweet.  \n",
    "    string = []\n",
    "    # Each time the REGEX matches, add the matched string to the running list of all words\n",
    "    for matches in my_extensible_tokenizer.findall(text[0]):\n",
    "        # Add matched string to list containing elements of current tweet\n",
    "        string.append(matches)\n",
    "        # If we are at least 2 words into this tweet...\n",
    "        if count2 >= 1:\n",
    "            # Store backward-looking 2-gram in variable with space concatenated in middle\n",
    "            gram2 = string[count2 - 1] + ' ' + string[count2]\n",
    "            # Check whether 2-gram is in dictionary\n",
    "            if gram2 in corp_2gram:\n",
    "                # If in dictionary, add to the entry's count\n",
    "                corp_2gram[gram2] += 1\n",
    "            # If not in entry, create new entry and set count to 1\n",
    "            else:\n",
    "                corp_2gram[gram2] = 1\n",
    "        ## Proceed equivalently for 3-, 4-, and 5-gram dictionaries: check\n",
    "        ## whether we are sufficiently far into the tweet to form a backward-looking\n",
    "        ## gram of that size, save the gram, and either add to that gram's counter\n",
    "        ## in the dictionary or create new entry for that gram.\n",
    "        if count2 >= 2:\n",
    "            gram3 = string[count2 - 2] + ' ' + gram2\n",
    "            if gram3 in corp_3gram:\n",
    "                corp_3gram[gram3] += 1\n",
    "            else:\n",
    "                corp_3gram[gram3] = 1\n",
    "                \n",
    "        if count2 >= 3:\n",
    "            gram4 = string[count2 - 3] + ' ' + gram3\n",
    "            if gram4 in corp_4gram:\n",
    "                corp_4gram[gram4] += 1\n",
    "            else:\n",
    "                corp_4gram[gram4] = 1\n",
    "        \n",
    "        if count2 >= 4:\n",
    "            gram5 = string[count2 - 4] + ' ' + gram4\n",
    "            if gram5 in corp_5gram:\n",
    "                corp_5gram[gram5] += 1\n",
    "            else:\n",
    "                corp_5gram[gram5] = 1\n",
    "                \n",
    "        count2 += 1\n",
    "        \n",
    "         #just in case get empty matches\n",
    "        if matches != '':\n",
    "            # Add string to master list\n",
    "            temp.append(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dictionary that will hold counts of all distinct tokens\n",
    "corpus = {}\n",
    "\n",
    "# Define variable that will hold counts of each word length\n",
    "lengths = []\n",
    "\n",
    "# Looking through entire list of (repeated) words, count instances of distinct words\n",
    "for word in temp:\n",
    "    # Add character count (non-whitespace) to counting list\n",
    "    lengths.append(len(word))\n",
    "    # If word has already been seen, add one to its count\n",
    "    if word in corpus:\n",
    "        corpus[word] += 1\n",
    "    # If word has not already been seen, add word to list\n",
    "    else:\n",
    "        corpus[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## temp contains urls, which are included in the count.  \n",
    "## Are urls really words?\n",
    "print(\"Number of distinct words: \" + str(len(corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average number of characters per tweet:\" + str(data[\"characters\"].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"characters\"] = 0\n",
    "# Write character count of each tweet to the character count column\n",
    "data[\"characters\"] = data[\"tweet\"].str.len()\n",
    "print(\"Average number of characters per tweet:\" + str(data[\"characters\"].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Avg. number of words/tweet is \" + str(len(temp)/data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average number of characters per token: \" + str(sum(lengths)/len(temp)))\n",
    "print(\"Standard deviation of characters per token: \" + str(stdev(lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dictionary of words and counts to dataframe to enable sorting by count\n",
    "corpus_df = pd.DataFrame(list(corpus.items()), columns = ['word', 'count'])\n",
    "# Sort by count, in descending fashion\n",
    "corpus_df = corpus_df.sort_values(by=['count'],ascending=False)\n",
    "# Sum counts from first 10 rows in sorted dataframe\n",
    "Top10 = sum(corpus_df['count'][0:9])\n",
    "print(\"Total number of times that 10 most popular tokens appear: \" + str(Top10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Token/type ratio???????\n",
    "print(\"Average number of times a specific word is seen (i.e. average number of tokens per type) is\", len(temp)/len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nDistinct word 2-grams:' + str(len(corp_2gram)))\n",
    "print('\\nDistinct word 3-grams:' + str(len(corp_3gram)))\n",
    "print('\\nDistinct word 4-grams:' + str(len(corp_4gram)))\n",
    "print('\\nDistinct word 5-grams:' + str(len(corp_5gram)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nDistinct char 2-grams:' + str(len(n_2)))\n",
    "print('\\nDistinct char 3-grams:' + str(len(n_3)))\n",
    "print('\\nDistinct char 4-grams:' + str(len(n_4)))\n",
    "print('\\nDistinct char 5-grams:' + str(len(n_5)))\n",
    "print('\\nDistinct char 6-grams:' + str(len(n_6)))\n",
    "print('\\nDistinct char 7-grams:' + str(len(n_7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Plot of token log frequency????????????\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in and run tokenizer on all four gold datasets - train, dev, devtest, and test \n",
    "dev = pd.read_csv(\"Gold/dev.txt\", sep='\\t', header=None, \n",
    "                   names=['id', 'class', 'tweet'], encoding='utf-8')\n",
    "dev.drop(['id'], axis=1, inplace=True)\n",
    "\n",
    "train = pd.read_csv(\"Gold/train.txt\", sep='\\t', header=None, \n",
    "                   names=['id', 'class', 'tweet'], encoding='utf-8')\n",
    "train.drop(['id'], axis=1, inplace=True)\n",
    "\n",
    "devtest = pd.read_csv(\"Gold/devtest.txt\", sep='\\t', header=None, \n",
    "                   names=['id', 'class', 'tweet'], encoding='utf-8')\n",
    "devtest.drop(['id'], axis=1, inplace=True)\n",
    "\n",
    "test = pd.read_csv(\"Gold/test.txt\", sep='\\t', header=None, \n",
    "                   names=['id', 'class', 'tweet'], encoding='utf-8')\n",
    "test.drop(['id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list that will hold all token types for Gold dev data\n",
    "temp_gold_dev = []\n",
    "\n",
    "for text in dev['tweet'].values:\n",
    "    # Each time the REGEX matches, add the matched string to the running list of all words\n",
    "    for matches in my_extensible_tokenizer.findall(text):\n",
    "         #just in case get empty matches\n",
    "        if matches != '':\n",
    "            # Add string to master list\n",
    "            temp_gold_dev.append(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create null dictionaries that will store types and counts for each dataset\n",
    "corpus_gold_dev = {}\n",
    "corpus_gold_train = {}\n",
    "corpus_gold_devtest = {}\n",
    "corpus_gold_test = {}\n",
    "# Create null dictionary that will store types and counts for all gold datasets combined\n",
    "corpus_gold = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking through entire list of (repeated) words, count instances of distinct words\n",
    "for word in temp_gold_train:\n",
    "    # If word has already been seen, add one to its count\n",
    "    if word in corpus_gold_train:\n",
    "        corpus_gold_train[word] += 1\n",
    "    # If word has not already been seen, add word to list\n",
    "    else:\n",
    "        corpus_gold_train[word] = 1\n",
    "        \n",
    "    if word in corpus_gold:\n",
    "        corpus_gold[word] += 1\n",
    "    else:\n",
    "        corpus_gold[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking through entire list of (repeated) words, count instances of distinct words\n",
    "for word in temp_gold_devtest:\n",
    "    # If word has already been seen, add one to its count\n",
    "    if word in corpus_gold_devtest:\n",
    "        corpus_gold_devtest[word] += 1\n",
    "    # If word has not already been seen, add word to list\n",
    "    else:\n",
    "        corpus_gold_devtest[word] = 1\n",
    "\n",
    "    if word in corpus_gold:\n",
    "        corpus_gold[word] += 1\n",
    "    else:\n",
    "        corpus_gold[word] = 1     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking through entire list of (repeated) words, count instances of distinct words\n",
    "for word in temp_gold_test:\n",
    "    # If word has already been seen, add one to its count\n",
    "    if word in corpus_gold_test:\n",
    "        corpus_gold_test[word] += 1\n",
    "    # If word has not already been seen, add word to list\n",
    "    else:\n",
    "        corpus_gold_test[word] = 1\n",
    "\n",
    "    if word in corpus_gold:\n",
    "        corpus_gold[word] += 1\n",
    "    else:\n",
    "        corpus_gold[word] = 1     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variable that will store values found in exclusively the dev data\n",
    "justdev = corpus_gold_dev.keys() - corpus_gold_train.keys()\n",
    "print('Number of word types found in dev data but not in training data: ' + str(len(justdev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique words in dev set\n",
    "unique1 = len(corpus_gold)\n",
    "\n",
    "# Number of unique words in training set\n",
    "unique2 = len(corpus_gold)\n",
    "\n",
    "# Number of unique words in devtest set\n",
    "unique3 = len(corpus_gold)\n",
    "\n",
    "# Number of unique words in test set\n",
    "unique4 = len(corpus_gold)\n",
    "\n",
    "# Save number of tokens in dev set\n",
    "full1 = len(temp_gold_dev)\n",
    "# Save number of tokens in training set        \n",
    "full2 = len(temp_gold_train)   \n",
    "# Save number of tokens in devset   \n",
    "full3 = len(temp_gold_devtest) \n",
    "# Save number of tokens in test set\n",
    "full4 = len(temp_gold_test)\n",
    "\n",
    "lexicon = pd.DataFrame({'unique':[0,unique1,unique2,unique3,unique4],'total':[0,full1,full1+full2,full1+full2+full3,full1+full2+full3+full4]})\n",
    "lexicon.plot(x ='total', y='unique', kind = 'line', title='Gold Dataset',marker='.', markersize=15,legend=None)\n",
    "plt.xlabel(\"Total Number of Words\")\n",
    "plt.ylabel(\"Unique Words in Lexicon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = train['class'].value_counts()\n",
    "classes2 = dev['class'].value_counts()\n",
    "\n",
    "print('Tweet class distribution is as follows:\\n')\n",
    "print(str(classes.values[0] + classes2.values[0]) + ' ' + classes.index[0] + ' tweets.\\n')\n",
    "print(str(classes.values[1] + classes2.values[1]) + ' ' + classes.index[1] + ' tweets.\\n')\n",
    "print(str(classes.values[2] + classes2.values[2]) + ' ' + classes.index[2] + ' tweets.\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at top word types across three classes\n",
    "positive = train[train['class'] == 'positive']\n",
    "neutral = train[train['class'] == 'neutral']\n",
    "negative = train[train['class'] == 'negative']\n",
    "temp_pos = []\n",
    "\n",
    "for text in positive['tweet'].values:\n",
    "    # Each time the REGEX matches, add the matched string to the running list of all words\n",
    "    for matches in my_extensible_tokenizer.findall(text):\n",
    "         #just in case get empty matches\n",
    "        if matches != '':\n",
    "            # Add string to master list\n",
    "            temp_pos.append(matches)\n",
    "\n",
    "\n",
    "temp_neu = []\n",
    "\n",
    "for text in neutral['tweet'].values:\n",
    "    # Each time the REGEX matches, add the matched string to the running list of all words\n",
    "    for matches in my_extensible_tokenizer.findall(text):\n",
    "         #just in case get empty matches\n",
    "        if matches != '':\n",
    "            # Add string to master list\n",
    "            temp_neu.append(matches)\n",
    "            \n",
    "temp_neg = []\n",
    "\n",
    "for text in negative['tweet'].values:\n",
    "    # Each time the REGEX matches, add the matched string to the running list of all words\n",
    "    for matches in my_extensible_tokenizer.findall(text):\n",
    "         #just in case get empty matches\n",
    "        if matches != '':\n",
    "            # Add string to master list\n",
    "            temp_neg.append(matches)\n",
    "\n",
    "corpus_pos = {}\n",
    "\n",
    "# Looking through entire list of (repeated) words, count instances of distinct words\n",
    "for word in temp_pos:\n",
    "    # If word has already been seen, add one to its count\n",
    "    if word in corpus_pos:\n",
    "        corpus_pos[word] += 1\n",
    "    # If word has not already been seen, add word to list\n",
    "    else:\n",
    "        corpus_pos[word] = 1\n",
    "    \n",
    "pos_df = pd.DataFrame(list(corpus_pos.items()), columns = ['word', 'count'])\n",
    "# Sort by count, in descending fashion\n",
    "pos_df = pos_df.sort_values(by=['count'],ascending=False)\n",
    "\n",
    "\n",
    "    \n",
    "corpus_neu = {}\n",
    "\n",
    "# Looking through entire list of (repeated) words, count instances of distinct words\n",
    "for word in temp_neu:\n",
    "    # If word has already been seen, add one to its count\n",
    "    if word in corpus_neu:\n",
    "        corpus_neu[word] += 1\n",
    "    # If word has not already been seen, add word to list\n",
    "    else:\n",
    "        corpus_neu[word] = 1\n",
    "\n",
    "neu_df = pd.DataFrame(list(corpus_neu.items()), columns = ['word', 'count'])\n",
    "# Sort by count, in descending fashion\n",
    "neu_df = neu_df.sort_values(by=['count'],ascending=False)\n",
    "\n",
    "\n",
    "corpus_neg = {}\n",
    "\n",
    "# Looking through entire list of (repeated) words, count instances of distinct words\n",
    "for word in temp_neg:\n",
    "    # If word has already been seen, add one to its count\n",
    "    if word in corpus_neg:\n",
    "        corpus_neg[word] += 1\n",
    "    # If word has not already been seen, add word to list\n",
    "    else:\n",
    "        corpus_neg[word] = 1\n",
    "        \n",
    "neg_df = pd.DataFrame(list(corpus_neg.items()), columns = ['word', 'count'])\n",
    "# Sort by count, in descending fashion\n",
    "neg_df = neg_df.sort_values(by=['count'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training dictionary to dataframe so that it can be ordered by count\n",
    "train_df = pd.DataFrame(list(corpus_gold_train.items()), columns = ['word', 'count'])\n",
    "# Sort by count, in descending fashion\n",
    "train_df = train_df.sort_values(by=['count'],ascending=False)\n",
    "\n",
    "# Save dev dictionary to dataframe so that it can be ordered by count\n",
    "dev_df = pd.DataFrame(list(corpus_gold_dev.items()), columns = ['word', 'count'])\n",
    "# Sort by count, in descending fashion\n",
    "dev_df = dev_df.sort_values(by=['count'],ascending=False)\n",
    "\n",
    "Top60 = pd.DataFrame()\n",
    "Top60['train'] = train_df['word'].values[0:60]\n",
    "Top60['dev'] = dev_df['word'].values[0:60]\n",
    "Top60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct dictionaries that will contain distinct character n-grams\n",
    "n_2 = {}\n",
    "n_3 = {}\n",
    "n_4 = {}\n",
    "n_5 = {}\n",
    "n_6 = {}\n",
    "n_7 = {}\n",
    "\n",
    "for tweet in data.values:\n",
    "    # Add to total character count\n",
    "    num += len(tweet[0])\n",
    "    \n",
    "    # Moving through each letter of the tweet...\n",
    "    # Creating distinct character n-grams\n",
    "    for i in range(1,len(tweet[0])):\n",
    "        # Form two-character combos (i.e. n-2 grams)\n",
    "        string2 = tweet[0][i-1:i+1]\n",
    "        # Check whether n-2 gram is already in corpus\n",
    "        if string2 in n_2:\n",
    "            # If already in corpus, add to count\n",
    "            n_2[string2] += 1\n",
    "        # If n-2 gram has not already been seen, add to list\n",
    "        else:\n",
    "            n_2[string2] = 1\n",
    "        # For situations where we are at least 3 characters away from the end-character   \n",
    "        if len(tweet[0]) - i >= 2:\n",
    "            # Form three-character combo\n",
    "            string3 = tweet[0][i-1:i+2]\n",
    "            # Check whether n-3 gram is already in corpus\n",
    "            if string3 in n_3:\n",
    "                # If already in corpus, add to count\n",
    "                n_3[string3] += 1\n",
    "            # If n-3 has not already been seen, add to list\n",
    "            else:\n",
    "                n_3[string3] = 1\n",
    "                \n",
    "        ## Continue equivalently for n-4, n-5, n-6, n-7: check whether we're far\n",
    "        ## enough away from end of tweet to form forward-looking string of that size,\n",
    "        ## save the string, and either save new dict entry or add to dict counter\n",
    "        if len(tweet[0]) - i >= 3:\n",
    "            string4 = tweet[0][i-1:i+3]\n",
    "\n",
    "            if string4 in n_4:\n",
    "                n_4[string4] += 1\n",
    "            # If n-4 gram has not already been seen, add to list\n",
    "            else:\n",
    "                n_4[string4] = 1\n",
    "                \n",
    "        if len(tweet[0]) - i >= 4:\n",
    "            string5 = tweet[0][i-1:i+4]\n",
    "\n",
    "            if string5 in n_5:\n",
    "                n_5[string5] += 1\n",
    "            # If n-5 gram has not already been seen, add to list\n",
    "            else:\n",
    "                n_5[string5] = 1\n",
    "            \n",
    "        if len(tweet[0]) - i >= 5:\n",
    "            string6 = tweet[0][i-1:i+5]\n",
    "\n",
    "            if string6 in n_6:\n",
    "                n_6[string6] += 1\n",
    "            # If n-6 gram has not already been seen, add to list\n",
    "            else:\n",
    "                n_6[string6] = 1\n",
    "                \n",
    "        if len(tweet[0]) - i >= 6:\n",
    "            string7 = tweet[0][i-1:i+6]\n",
    "\n",
    "            if string7 in n_7:\n",
    "                n_7[string7] += 1\n",
    "            # If n-7 gram has not already been seen, add to list\n",
    "            else:\n",
    "                n_7[string7] = 1     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
