{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1\n",
    "#### Norman Hong, Ryan Callahan, Nora Wu and Masha Gubenko\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 Exploratory Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blocks below summarize statistical characteristics of the dev and gold datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import re \n",
    "from statistics import stdev\n",
    "import pandas as pd \n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from math import log\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import nltk \n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Read in the data\n",
    "data = pd.read_csv(\"Dev/INPUT.txt\", sep='\\t', header=None, \n",
    "                   names=['id', 'target', 'tweet'], encoding='utf-8')\n",
    "data.drop(['id', 'target'], axis=1, inplace=True)\n",
    "\n",
    "# Tokenize\n",
    "regexes=(\n",
    "# Keep usernames together (any token starting with @, followed by A-Z, a-z, 0-9)        \n",
    "r\"(?:@[\\w_]+)\",\n",
    "            \n",
    "# Keep hashtags together (any token starting with #, followed by A-Z, a-z, 0-9, _, or -)\n",
    "r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\",\n",
    "\n",
    "# abbreviations, e.g. U.S.A.\n",
    "r'(?:[A-Z]\\.)+',\n",
    "r'[A-Za-z]\\.(?:[A-Za-z0-9]\\.)+',\n",
    "r'[A-Z][bcdfghj-np-tvxz]+\\.',\n",
    "\n",
    "# URL, e.g. https://google.com\n",
    "r'https?:\\/\\/(?:www\\.',\n",
    "r'(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}',\n",
    "r'www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}',\n",
    "r'https?:\\/\\/(?:www\\.',\n",
    "r'(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}',\n",
    "r'www\\.[a-zA-Z0-9]+\\.[^\\s]{2,}',\n",
    "\n",
    "# Currency and percentages, e.g. $12.40, 82%\n",
    "r'\\$?\\d+(?:\\.\\d+)?%?',\n",
    "\n",
    "# Numbers i.e. 123,56.34\n",
    "r'(?:[0-9]+[,]?)+(?:[.][0-9]+)?',\n",
    "\n",
    "# Keep words with apostrophes, hyphens, and underscores together\n",
    "r\"(?:[a-z][a-zâ€™'\\-_]+[a-z])\",\n",
    "\n",
    "# Keep all other sequences of A-Z, a-z, 0-9, _ together\n",
    "r\"(?:[\\w_]+)\",\n",
    "\n",
    "# Match words at the end of a sentence.  e.g. tree. or tree!\n",
    "r'(?:[a-z]+(?=[.!\\?]))',\n",
    "\n",
    "# Everything else that's not whitespace\n",
    "# It seems like this captures punctuations and emojis and emoticons.  \n",
    "#r\"(?:\\S)\"\n",
    ")\n",
    "\n",
    "big_regex=\"|\".join(regexes)\n",
    "my_extensible_tokenizer = re.compile(big_regex, re.VERBOSE | re.I | re.UNICODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  Input Data\n",
    "Total number of tweets is 11,906.  \n",
    "Total number of characters is 1,354,375.  \n",
    "Total number of distinct words (vocabulary) is 41,760.  \n",
    "Average number of words and characters per tweet is 16.066 and 113.756, respectively.  \n",
    "Average number and standard deviation of characters and token 5.979 and 4.706. \n",
    "Total tokens corresponding to 10 most frequent words is 23,268.  \n",
    "Number of distinct word n-grams for n=2,3,4,5:\n",
    "- n-2: 130,116\n",
    "- n-3: 157,675\n",
    "- n-4: 153,193\n",
    "- n-5: 142,858\n",
    "\n",
    "Number of distinct character n-grams for n=2,3,4,5,6,7:\n",
    "- n-2: 8,069\n",
    "- n-3: 98,192\n",
    "- n-4: 212,264\n",
    "- n-5: 361,987\n",
    "- n-6: 520,197\n",
    "- n-7: 662,468   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Total number of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of tweets: \" + str(data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. The total number of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 0\n",
    "for tweet in data.values:\n",
    "    # Add to total character count\n",
    "    num += len(tweet[0])\n",
    "print(\"Total number of characters, including spaces: \" + str(num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. The total number of distinct words (vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list that will hold all tokens\n",
    "temp = []\n",
    "\n",
    "# Iterate through each tweet\n",
    "for text in data.values:\n",
    "    # Each time the REGEX matches, add the matched string to the running list of all words\n",
    "    for matches in my_extensible_tokenizer.findall(text[0]):       \n",
    "         #just in case get empty matches\n",
    "        if matches != '':\n",
    "            # Add string to master list\n",
    "            temp.append(matches)\n",
    "            \n",
    "# Define dictionary that will hold counts of all distinct tokens\n",
    "corpus = {}\n",
    "\n",
    "# Define variable that will hold counts of each word length\n",
    "lengths = []\n",
    "\n",
    "# Looking through entire list of (repeated) words, count instances of distinct words\n",
    "for word in temp:\n",
    "    # Add character count (non-whitespace) to counting list\n",
    "    lengths.append(len(word))\n",
    "    # If word has already been seen, add one to its count\n",
    "    if word in corpus:\n",
    "        corpus[word] += 1\n",
    "    # If word has not already been seen, add word to list\n",
    "    else:\n",
    "        corpus[word] = 1\n",
    "        \n",
    "## temp contains urls, which are included in the count.  \n",
    "## Are urls really words?\n",
    "print(\"Number of distinct words: \" + str(len(corpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. The average number of characters and words in each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Avgerage number of words per tweet is \" + str(len(temp)/data.shape[0]))\n",
    "data[\"characters\"] = 0\n",
    "# Write character count of each tweet to the character count column\n",
    "data[\"characters\"] = data[\"tweet\"].str.len()\n",
    "print(\"Average number of characters per tweet is\" + str(data[\"characters\"].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. The average number and standard deviation of characters per token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average number of characters per token: \" + str(sum(lengths)/len(temp)))\n",
    "print(\"Standard deviation of characters per token: \" + str(stdev(lengths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. The total number of tokens corresponding to the top 10 most frequent words (types) in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dictionary of words and counts to dataframe to enable sorting by count\n",
    "corpus_df = pd.DataFrame(list(corpus.items()), columns = ['word', 'count'])\n",
    "# Sort by count, in descending fashion\n",
    "corpus_df = corpus_df.sort_values(by=['count'],ascending=False)\n",
    "# Sum counts from first 10 rows in sorted dataframe\n",
    "Top10 = sum(corpus_df['count'][0:9])\n",
    "print(\"Total number of times that 10 most popular tokens appear: \" + str(Top10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7. The token/type ratio in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average number of times a specific word is seen (i.e. average number of tokens per type) is\", len(temp)/len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. The total number of distinct n-grams (of words) that appear in the dataset for n=2,3,4,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GramCount(data,n):\n",
    "    i = 0\n",
    "    ngramlist = [] \n",
    "    while i < len(data):\n",
    "        tokens = my_extensible_tokenizer.findall(data['tweet'][i])\n",
    "        grams = list(ngrams(tokens, n)) \n",
    "        for gram in grams:\n",
    "            ngramlist.append(gram)\n",
    "        i += 1\n",
    "    return(len(set(ngramlist)))\n",
    "\n",
    "for i in range(2,6):    \n",
    "      print(\"There are \", GramCount(data,i), \" of distinct \", i,\"-grams.\",sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. The total number of distinct n-grams of characters that appear for n=2,3,4,5,6,7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct dictionaries that will contain distinct character n-grams\n",
    "n_2 = {}\n",
    "n_3 = {}\n",
    "n_4 = {}\n",
    "n_5 = {}\n",
    "n_6 = {}\n",
    "n_7 = {}\n",
    "\n",
    "# 2. Total number of characters.\n",
    "# 1354375\n",
    "\n",
    "num = 0\n",
    "# Moving through all tweets in the data\n",
    "for tweet in data.values:\n",
    "    # Add to total character count\n",
    "    num += len(tweet[0])\n",
    "    \n",
    "    # Moving through each letter of the tweet...\n",
    "    # Creating distinct character n-grams\n",
    "    for i in range(1,len(tweet[0])):\n",
    "        # Form two-character combos (i.e. n-2 grams)\n",
    "        string2 = tweet[0][i-1:i+1]\n",
    "        # Check whether n-2 gram is already in corpus\n",
    "        if string2 in n_2:\n",
    "            # If already in corpus, add to count\n",
    "            n_2[string2] += 1\n",
    "        # If n-2 gram has not already been seen, add to list\n",
    "        else:\n",
    "            n_2[string2] = 1\n",
    "        # For situations where we are at least 3 characters away from the end-character   \n",
    "        if len(tweet[0]) - i >= 2:\n",
    "            # Form three-character combo\n",
    "            string3 = tweet[0][i-1:i+2]\n",
    "            # Check whether n-3 gram is already in corpus\n",
    "            if string3 in n_3:\n",
    "                # If already in corpus, add to count\n",
    "                n_3[string3] += 1\n",
    "            # If n-3 has not already been seen, add to list\n",
    "            else:\n",
    "                n_3[string3] = 1\n",
    "                \n",
    "        ## Continue equivalently for n-4, n-5, n-6, n-7: check whether we're far\n",
    "        ## enough away from end of tweet to form forward-looking string of that size,\n",
    "        ## save the string, and either save new dict entry or add to dict counter\n",
    "        if len(tweet[0]) - i >= 3:\n",
    "            string4 = tweet[0][i-1:i+3]\n",
    "\n",
    "            if string4 in n_4:\n",
    "                n_4[string4] += 1\n",
    "            # If n-4 gram has not already been seen, add to list\n",
    "            else:\n",
    "                n_4[string4] = 1\n",
    "                \n",
    "        if len(tweet[0]) - i >= 4:\n",
    "            string5 = tweet[0][i-1:i+4]\n",
    "\n",
    "            if string5 in n_5:\n",
    "                n_5[string5] += 1\n",
    "            # If n-5 gram has not already been seen, add to list\n",
    "            else:\n",
    "                n_5[string5] = 1\n",
    "            \n",
    "        if len(tweet[0]) - i >= 5:\n",
    "            string6 = tweet[0][i-1:i+5]\n",
    "\n",
    "            if string6 in n_6:\n",
    "                n_6[string6] += 1\n",
    "            # If n-6 gram has not already been seen, add to list\n",
    "            else:\n",
    "                n_6[string6] = 1\n",
    "                \n",
    "        if len(tweet[0]) - i >= 6:\n",
    "            string7 = tweet[0][i-1:i+6]\n",
    "\n",
    "            if string7 in n_7:\n",
    "                n_7[string7] += 1\n",
    "            # If n-7 gram has not already been seen, add to list\n",
    "            else:\n",
    "                n_7[string7] = 1  \n",
    "\n",
    "print('There are ' + str(len(n_2)) + ' of distinct char 2-grams')\n",
    "print('There are ' + str(len(n_3)) + ' of distinct char 3-grams')\n",
    "print('There are ' + str(len(n_4)) + ' of distinct char 4-grams')\n",
    "print('There are ' + str(len(n_5)) + ' of distinct char 5-grams')\n",
    "print('There are ' + str(len(n_6)) + ' of distinct char 6-grams')\n",
    "print('There are ' + str(len(n_7)) + ' of distinct char 7-grams')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10. Plot a token log frequency. Describe what this plot means and how to interpret it. Describe out it might help you understand coverage when training a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot below shows that a huge majority of words appear with low frequency (n = 1, log(n) = 0). A few words comprise a disproportionately large percentage of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain frequency values from dict\n",
    "tok_freqs = list(corpus.values())\n",
    "# Take log of all freq counts\n",
    "log_freqs = [log(x) for x in tok_freqs]\n",
    "# Sort form least to greatest for use in plot\n",
    "log_freqs.sort()\n",
    "# Save list as series to enable plotting\n",
    "logz = pd.Series(log_freqs)\n",
    "# Plot log(freq) of each token, sorted from smallest to largest\n",
    "logz.plot(use_index=False,title='Token Log Frequency')\n",
    "plt.xlabel(\"index(sorted)\")\n",
    "plt.ylabel(\"Log(Frequency)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Gold Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of word types that appear in dev data but not training data is 5,062.  \n",
    "Token type ratio is 4.58. \n",
    "\n",
    "The number of distinct words in the input dataset is 41,760.\n",
    "The number of distinct words in the gold dataset is 61,727.\n",
    "\n",
    "The number of positive tweets in the gold dataset is 3,017.\n",
    "The number of neutral tweets in the gold dataset is 2,001.\n",
    "The number of negative tweets in the gold dataset is 850.\n",
    "\n",
    "There is a lot of similarity between the the top words across the three classes. However, we would like to note that \"Donald\", \"Trump\", \"Erdogan\", and \"Jeb\" appear only in the top 50 of the negative tweets. \"Tomorrow\" appears more frequently as tweets get more positive. \"But\" appears less frequently as tweets get more positive. \"Friday\" and \"Jurassic\" appear only in the top 50 of the positive tweets.\"Apple\" appears only in the top 50 of the positive and neutral tweets. Also, \"Amazon\" and \"Prime\" both appear higher in the list of negative tweet frequency than \"Amazon\" does in the list of positive tweet frequencies.\n",
    "\n",
    "The Dev dataset has frequent occurrences of Obama, SCOTUS, Sunday, Minecraft, Snoop, Rick, Sarah, planned, Ric, Palin, Dogg, Netflix, Nike, Serena, and Michelle, which suggests a larger political/pop culture theme for the dev dataset.\n",
    "The training dataset has frequent occurrences of Amazon, Friday, Apple, and night. The two sets are otherwise fairly similar, but the frequent words in the training set more closely match those seen in the overall positive, neutral, and negative subsets, suggesting that the training set is either substantially larger or less niche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in the data\n",
    "data_dev = pd.read_csv(\"Gold/dev.txt\", sep='\\t', header=None, index_col=False,\n",
    "                   names=['id', 'target', 'tweet'], encoding='utf-8')\n",
    "data_dev.drop(['id'], axis=1, inplace=True)\n",
    "\n",
    "data_train = pd.read_csv(\"Gold/train.txt\", sep='\\t', header=None, index_col=False,\n",
    "                   names=['id', 'target', 'tweet'], encoding='utf-8')\n",
    "data_train.drop(['id'], axis=1, inplace=True)\n",
    "\n",
    "data_devtest = pd.read_csv(\"Gold/devtest.txt\", sep='\\t', header=None, index_col=False,\n",
    "                           names=['id', 'target', 'tweet'], encoding='utf-8')\n",
    "data_devtest.drop(['id'], axis=1, inplace=True)\n",
    "\n",
    "data_test = pd.read_csv(\"Gold/test.txt\", sep='\\t', header=None, index_col=False,\n",
    "                           names=['id', 'target', 'tweet'], encoding='utf-8')\n",
    "data_test.drop(['id'], axis=1, inplace=True)\n",
    "\n",
    "# Define list that will hold all token types for Gold dev data\n",
    "temp_gold_dev = []\n",
    "\n",
    "for text in data_dev['tweet'].values:\n",
    "    # Each time the REGEX matches, add the matched string to the running list of all words\n",
    "    for matches in my_extensible_tokenizer.findall(text):\n",
    "         #just in case get empty matches\n",
    "        if matches != '':\n",
    "            # Add string to master list\n",
    "            temp_gold_dev.append(matches)\n",
    "            \n",
    "# Define list that will hold all token types for Gold training data\n",
    "temp_gold_train = []\n",
    "\n",
    "for text in data_train['tweet'].values:\n",
    "    # Each time the REGEX matches, add the matched string to the running list of all words\n",
    "    for matches in my_extensible_tokenizer.findall(text):\n",
    "         #just in case get empty matches\n",
    "        if matches != '':\n",
    "            # Add string to master list\n",
    "            temp_gold_train.append(matches)\n",
    "         \n",
    "                        \n",
    "# Define list that will hold all token types for Gold DEVTEST data\n",
    "temp_gold_devtest = []\n",
    "\n",
    "for text in data_devtest['tweet'].values:\n",
    "    # Each time the REGEX matches, add the matched string to the running list of all words\n",
    "    for matches in my_extensible_tokenizer.findall(text):\n",
    "         #just in case get empty matches\n",
    "        if matches != '':\n",
    "            # Add string to master list\n",
    "            temp_gold_devtest.append(matches)\n",
    "     \n",
    "        \n",
    "# Define list that will hold all token types for Gold test data\n",
    "temp_gold_test = []\n",
    "\n",
    "for text in data_test['tweet'].values:\n",
    "    # Each time the REGEX matches, add the matched string to the running list of all words\n",
    "    for matches in my_extensible_tokenizer.findall(text):\n",
    "         #just in case get empty matches\n",
    "        if matches != '':\n",
    "            # Add string to master list\n",
    "            temp_gold_test.append(matches)\n",
    "\n",
    "\n",
    "# Create null dictionaries that will store types and counts for each dataset\n",
    "corpus_gold_dev = {}\n",
    "corpus_gold_train = {}\n",
    "corpus_gold_devtest = {}\n",
    "corpus_gold_test = {}\n",
    "# Create null dictionary that will store types and counts for all gold datasets combined\n",
    "corpus_gold = {}\n",
    "\n",
    "# Looking through entire list of (repeated) words, count instances of distinct words\n",
    "for word in temp_gold_dev:\n",
    "    # If word has already been seen, add one to its count\n",
    "    if word in corpus_gold_dev:\n",
    "        corpus_gold_dev[word] += 1\n",
    "    # If word has not already been seen, add word to list\n",
    "    else:\n",
    "        corpus_gold_dev[word] = 1\n",
    "    if word in corpus_gold:\n",
    "        corpus_gold[word] += 1\n",
    "    else:\n",
    "        corpus_gold[word] = 1\n",
    "# Number of unique words in dev set\n",
    "unique1 = len(corpus_gold)\n",
    "\n",
    "# Looking through entire list of (repeated) words, count instances of distinct words\n",
    "for word in temp_gold_train:\n",
    "    # If word has already been seen, add one to its count\n",
    "    if word in corpus_gold_train:\n",
    "        corpus_gold_train[word] += 1\n",
    "    # If word has not already been seen, add word to list\n",
    "    else:\n",
    "        corpus_gold_train[word] = 1\n",
    "        \n",
    "    if word in corpus_gold:\n",
    "        corpus_gold[word] += 1\n",
    "    else:\n",
    "        corpus_gold[word] = 1\n",
    "# Number of unique words in training set\n",
    "unique2 = len(corpus_gold)\n",
    "\n",
    "# Looking through entire list of (repeated) words, count instances of distinct words\n",
    "for word in temp_gold_devtest:\n",
    "    # If word has already been seen, add one to its count\n",
    "    if word in corpus_gold_devtest:\n",
    "        corpus_gold_devtest[word] += 1\n",
    "    # If word has not already been seen, add word to list\n",
    "    else:\n",
    "        corpus_gold_devtest[word] = 1\n",
    "\n",
    "    if word in corpus_gold:\n",
    "        corpus_gold[word] += 1\n",
    "    else:\n",
    "        corpus_gold[word] = 1       \n",
    "# Number of unique words in devtest set\n",
    "unique3 = len(corpus_gold)\n",
    "\n",
    "# Looking through entire list of (repeated) words, count instances of distinct words\n",
    "for word in temp_gold_test:\n",
    "    # If word has already been seen, add one to its count\n",
    "    if word in corpus_gold_test:\n",
    "        corpus_gold_test[word] += 1\n",
    "    # If word has not already been seen, add word to list\n",
    "    else:\n",
    "        corpus_gold_test[word] = 1\n",
    "\n",
    "    if word in corpus_gold:\n",
    "        corpus_gold[word] += 1\n",
    "    else:\n",
    "        corpus_gold[word] = 1     \n",
    "# Number of unique words in test set\n",
    "unique4 = len(corpus_gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11. What is the number of types that appear in the dev data but not the training data (OOV)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variable that will store values found in exclusively the dev data\n",
    "justdev = corpus_gold_dev.keys() - corpus_gold_train.keys()\n",
    "print('Number of types found in dev data but not in training data: ' + str(len(justdev)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12. Look at the vocabulary growth (types) combining your four gold data sets against your input data. Plot vocabulary growth at difference sample sizes N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Distinct words in input dataset: \" + str(len(corpus)) + \"\\nDistinct words in gold dataset: \" + str(len(corpus_gold)))\n",
    "\n",
    "# Save number of tokens in dev set\n",
    "full1 = len(temp_gold_dev)\n",
    "# Save number of tokens in training set        \n",
    "full2 = len(temp_gold_train)   \n",
    "# Save number of tokens in devset   \n",
    "full3 = len(temp_gold_devtest) \n",
    "# Save number of tokens in test set\n",
    "full4 = len(temp_gold_test)\n",
    "\n",
    "lexicon = pd.DataFrame({'unique':[0,unique1,unique2,unique3,unique4],'total':[0,full1,full1+full2,full1+full2+full3,full1+full2+full3+full4]})\n",
    "lexicon.plot(x ='total', y='unique', kind = 'line', title='Gold Dataset',marker='.', markersize=15,legend=None)\n",
    "plt.xlabel(\"Total Number of Words\")\n",
    "plt.ylabel(\"Unique Words in Lexicon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 13. What is the class distribution of the training data set - how many negative, neutral, positive tweets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unique values of the class column for the training data set\n",
    "uniqgold = set(data_train['target']) \n",
    "\n",
    "numuniq = []\n",
    "for value in uniqgold:\n",
    "    count = 0\n",
    "    for value2 in data_train['target']:\n",
    "        if value2 == value:\n",
    "            count += 1\n",
    "    numuniq.append(count)\n",
    "\n",
    "uniqlist = {'Class':list(uniqgold), 'Number of Tweets': numuniq}\n",
    "uniqdf = pd.DataFrame(uniqlist)\n",
    "uniqdf   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 14. Look at the difference between the top word types across these three classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = data_train[data_train['target'] == 'positive']\n",
    "neutral = data_train[data_train['target'] == 'neutral']\n",
    "negative = data_train[data_train['target'] == 'negative']\n",
    "\n",
    "temp_pos = []\n",
    "for text in positive['tweet'].values:\n",
    "    # Each time the REGEX matches, add the matched string to the running list of all words\n",
    "    for matches in my_extensible_tokenizer.findall(text):\n",
    "         #just in case get empty matches\n",
    "        if matches != '':\n",
    "            # Add string to master list\n",
    "            temp_pos.append(matches)\n",
    "\n",
    "temp_neu = []\n",
    "for text in neutral['tweet'].values:\n",
    "    # Each time the REGEX matches, add the matched string to the running list of all words\n",
    "    for matches in my_extensible_tokenizer.findall(text):\n",
    "         #just in case get empty matches\n",
    "        if matches != '':\n",
    "            # Add string to master list\n",
    "            temp_neu.append(matches)\n",
    " \n",
    "temp_neg = []\n",
    "for text in negative['tweet'].values:\n",
    "    # Each time the REGEX matches, add the matched string to the running list of all words\n",
    "    for matches in my_extensible_tokenizer.findall(text):\n",
    "         #just in case get empty matches\n",
    "        if matches != '':\n",
    "            # Add string to master list\n",
    "            temp_neg.append(matches)\n",
    "\n",
    "corpus_pos = {}\n",
    "# Looking through entire list of (repeated) words, count instances of distinct words\n",
    "for word in temp_pos:\n",
    "    # If word has already been seen, add one to its count\n",
    "    if word in corpus_pos:\n",
    "        corpus_pos[word] += 1\n",
    "    # If word has not already been seen, add word to list\n",
    "    else:\n",
    "        corpus_pos[word] = 1\n",
    "    \n",
    "pos_df = pd.DataFrame(list(corpus_pos.items()), columns = ['word', 'count'])\n",
    "# Sort by count, in descending fashion\n",
    "pos_df = pos_df.sort_values(by=['count'],ascending=False)\n",
    "   \n",
    "corpus_neu = {}\n",
    "# Looking through entire list of (repeated) words, count instances of distinct words\n",
    "for word in temp_neu:\n",
    "    # If word has already been seen, add one to its count\n",
    "    if word in corpus_neu:\n",
    "        corpus_neu[word] += 1\n",
    "    # If word has not already been seen, add word to list\n",
    "    else:\n",
    "        corpus_neu[word] = 1\n",
    "\n",
    "neu_df = pd.DataFrame(list(corpus_neu.items()), columns = ['word', 'count'])\n",
    "# Sort by count, in descending fashion\n",
    "neu_df = neu_df.sort_values(by=['count'],ascending=False)\n",
    "\n",
    "\n",
    "corpus_neg = {}\n",
    "# Looking through entire list of (repeated) words, count instances of distinct words\n",
    "for word in temp_neg:\n",
    "    # If word has already been seen, add one to its count\n",
    "    if word in corpus_neg:\n",
    "        corpus_neg[word] += 1\n",
    "    # If word has not already been seen, add word to list\n",
    "    else:\n",
    "        corpus_neg[word] = 1\n",
    "\n",
    "neg_df = pd.DataFrame(list(corpus_neg.items()), columns = ['word', 'count'])\n",
    "# Sort by count, in descending fashion\n",
    "neg_df = neg_df.sort_values(by=['count'],ascending=False)\n",
    "\n",
    "Top51 = pd.DataFrame()\n",
    "Top51['pos'] = pos_df['word'].values[0:50]\n",
    "Top51['neu'] = neu_df['word'].values[0:50]\n",
    "Top51['neg'] = neg_df['word'].values[0:50]\n",
    "Top51"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 15. What words are particularly characteristic of your training set and dev set? Are they the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training dictionary to dataframe so that it can be ordered by count\n",
    "train_df = pd.DataFrame(list(corpus_gold_train.items()), columns = ['word', 'count'])\n",
    "# Sort by count, in descending fashion\n",
    "train_df = train_df.sort_values(by=['count'],ascending=False)\n",
    "\n",
    "# Save dev dictionary to dataframe so that it can be ordered by count\n",
    "dev_df = pd.DataFrame(list(corpus_gold_dev.items()), columns = ['word', 'count'])\n",
    "# Sort by count, in descending fashion\n",
    "dev_df = dev_df.sort_values(by=['count'],ascending=False)\n",
    "\n",
    "Top61 = pd.DataFrame()\n",
    "Top61['train'] = train_df['word'].values[0:60]\n",
    "Top61['dev'] = dev_df['word'].values[0:60]\n",
    "Top61"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Message Polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WRITE UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages\n",
    "import re \n",
    "import pandas as pd \n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "import nltk \n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Read in the data\n",
    "data_dev = pd.read_csv(\"Gold/dev.txt\", sep='\\t', header=None, index_col=False,\n",
    "                   names=['id', 'target', 'tweet'], encoding='utf-8')\n",
    "data_dev.drop(['id'], axis=1, inplace=True)\n",
    "\n",
    "data_train = pd.read_csv(\"Gold/train.txt\", sep='\\t', header=None, index_col=False,\n",
    "                   names=['id', 'target', 'tweet'], encoding='utf-8')\n",
    "data_train.drop(['id'], axis=1, inplace=True)\n",
    "\n",
    "data_devtest = pd.read_csv(\"Gold/devtest.txt\", sep='\\t', header=None, index_col=False,\n",
    "                           names=['id', 'target', 'tweet'], encoding='utf-8')\n",
    "data_devtest.drop(['id'], axis=1, inplace=True)\n",
    "\n",
    "data_test = pd.read_csv(\"Gold/test.txt\", sep='\\t', header=None, index_col=False,\n",
    "                           names=['id', 'target', 'tweet'], encoding='utf-8')\n",
    "data_test.drop(['id'], axis=1, inplace=True)\n",
    "\n",
    "'''\n",
    "kaggle_data = a = pd.read_csv(\"Gold/kaggle_data.csv\", encoding='iso-8859-1',\n",
    "                              names=[\"target\", \"ids\", \"date\", \"flag\", \"user\", \"tweet\"])\n",
    "kaggle_data.drop(['ids','flag','date','user'],axis=1,inplace=True)\n",
    "kaggle_data.loc[kaggle_data.loc[:, 'target'] == 0, 'target'] = 'negative'\n",
    "kaggle_data.loc[kaggle_data.loc[:, 'target'] == 4, 'target'] = 'positive'\n",
    "kaggle_data = kaggle_data.sample(frac=1).reset_index(drop=True).loc[0:50000,:]\n",
    "\n",
    "data_dev = data_dev.append(kaggle_data, ignore_index=True)\n",
    "'''\n",
    "\n",
    "# Tokenize\n",
    "regexes=(\n",
    "# Keep usernames together (any token starting with @, followed by A-Z, a-z, 0-9)        \n",
    "r\"(?:@[\\w_]+)\",\n",
    "\n",
    "# Keep hashtags together (any token starting with #, followed by A-Z, a-z, 0-9, _, or -)\n",
    "r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\",\n",
    "\n",
    "# abbreviations, e.g. U.S.A.\n",
    "r'(?:[A-Z]\\.)+',\n",
    "r'[A-Za-z]\\.(?:[A-Za-z0-9]\\.)+',\n",
    "r'[A-Z][bcdfghj-np-tvxz]+\\.',\n",
    "\n",
    "# URL, e.g. https://google.com Ryans's url pattern.\n",
    "r'https?:\\/\\/w{0,3}\\.?[a-zA-Z0-9-]+\\.[a-zA-Z0-9]{1,6}\\/?[a-zA-Z0-9\\/=]*\\s*',\n",
    "r'w{3}\\.[a-zA-Z0-9]+\\.[a-zA-Z0-9\\/=]+\\s*',\n",
    "r'w{0:3}\\.?[a-zA-Z0-9]+\\.[a-zA-Z0-9\\/=]+\\/+[a-zA-Z0-9\\/=]*\\s*',\n",
    "r'[a-zA-Z0-9]+\\.com\\/?[a-zA-Z0-9\\/=]*\\s*',\n",
    "r'[a-zA-Z0-9]+\\.edu\\/?[a-zA-Z0-9\\/=]*\\s*',\n",
    "r'[a-zA-Z0-9]+\\.gov\\/?[a-zA-Z0-9\\/=]*\\s*',\n",
    "r'[a-zA-Z0-9]+\\.org\\/?[a-zA-Z0-9\\/=]*\\s*',\n",
    "\n",
    "# currency and percentages, e.g. $12.40, 82%\n",
    "r'\\$?\\d+(?:\\.\\d+)?%?',\n",
    "\n",
    "# Numbers i.e. 123,56.34\n",
    "r'(?:[0-9]+[,]?)+(?:[.][0-9]+)?',\n",
    "\n",
    "# Keep words with apostrophes, hyphens and underscores together\n",
    "r\"(?:[a-z][a-zâ€™'\\-_]+[a-z])\",\n",
    "\n",
    "# Keep all other sequences of A-Z, a-z, 0-9, _ together\n",
    "r\"(?:[\\w_]+)\",\n",
    "\n",
    "# Match words at the end of a sentence.  e.g. tree. or tree!\n",
    "r'(?:[a-z]+(?=[.!\\?]))',\n",
    "\n",
    "# Everything else that's not whitespace\n",
    "# It seems like this captures punctuations and emojis and emoticons.  \n",
    "#r\"(?:\\S)\"\n",
    ")\n",
    "big_regex = \"|\".join(regexes)\n",
    "my_extensible_tokenizer = re.compile(big_regex, re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "\n",
    "trash_pattern = (\n",
    "# URL, e.g. https://google.com\n",
    "# This pattern will match any url.  \n",
    "r'(https?:\\/\\/(?:www\\.',\n",
    "r'(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}',\n",
    "r'www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}',\n",
    "r'https?:\\/\\/(?:www\\.',\n",
    "r'(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}',\n",
    "r'www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})',\n",
    "\n",
    "# Numbers i.e. 123,56.34\n",
    "r'(?:[0-9]+[,]?)+(?:[.][0-9]+)?',\n",
    "\n",
    "# Keep usernames together (any token starting with @, followed by A-Z, a-z, 0-9)        \n",
    "r\"(?:@[\\w_]+)\"\n",
    ")\n",
    "big_trash_pattern = \"|\".join(trash_pattern)\n",
    "trash_tokenizer = re.compile(big_trash_pattern, re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "hashtag_pattern = (\n",
    "r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"        \n",
    ")\n",
    "hashtag_tokenizer = re.compile(hashtag_pattern, re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "url_pattern = (\n",
    "# URL, e.g. https://google.com\n",
    "# This pattern will match any url.   Ryan's URl pattern\n",
    "r'https?:\\/\\/w{0,3}\\.?[a-zA-Z0-9-]+\\.[a-zA-Z0-9]{1,6}\\/?[a-zA-Z0-9\\/=]*\\s*',\n",
    "r'w{3}\\.[a-zA-Z0-9]+\\.[a-zA-Z0-9\\/=]+\\s*',\n",
    "r'w{0:3}\\.?[a-zA-Z0-9]+\\.[a-zA-Z0-9\\/=]+\\/+[a-zA-Z0-9\\/=]*\\s*',\n",
    "r'[a-zA-Z0-9]+\\.com\\/?[a-zA-Z0-9\\/=]*\\s*',\n",
    "r'[a-zA-Z0-9]+\\.edu\\/?[a-zA-Z0-9\\/=]*\\s*',\n",
    "r'[a-zA-Z0-9]+\\.gov\\/?[a-zA-Z0-9\\/=]*\\s*',\n",
    "r'[a-zA-Z0-9]+\\.org\\/?[a-zA-Z0-9\\/=]*\\s*',\n",
    ")\n",
    "big_url_pattern = \"|\".join(url_pattern)\n",
    "url_tokenizer = re.compile(big_url_pattern, re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "# Create a list of stop words\n",
    "stops = list(stopwords.words('english')) \n",
    "stop_words = []\n",
    "for word in stops:\n",
    "    if word[-3:] != \"n't\":\n",
    "        if word[-1] != 'n':\n",
    "            stop_words.append(word)\n",
    "            \n",
    "# Create a list of negation words         \n",
    "negation_words = ['not', \n",
    "                  'no', \n",
    "                  'never',\n",
    "                  'none',\n",
    "                  'no one',\n",
    "                  'nobody',\n",
    "                  'nothing',\n",
    "                  'neither',\n",
    "                  'never',\n",
    "                  'hardly',\n",
    "                  'scarcely',\n",
    "                  'barely',\n",
    "                  \"doesn't\",\n",
    "                  \"isn't\",\n",
    "                  \"wasn't\",\n",
    "                  \"shouldn't\",\n",
    "                  \"wouldn't\",\n",
    "                  \"couldn't\",\n",
    "                  \"won't\",\n",
    "                  \"can't\",\n",
    "                  \"don't\"\n",
    "                  ]\n",
    "\n",
    "def randomOversampler(DataFrame):\n",
    "    # Assumption is that positive class always majority.  \n",
    "    # This is true for current datasets.  \n",
    "    neutral_tweets = DataFrame.loc[DataFrame['target'] == 'neutral', :]\n",
    "    negative_tweets = DataFrame.loc[DataFrame['target'] == 'negative', :]\n",
    "    neu_rows = neutral_tweets.shape[0]\n",
    "    neg_rows = negative_tweets.shape[0]\n",
    "    pos_class_count = DataFrame.groupby(by='target').count().loc['positive',:][0]\n",
    "    neg_class_count = DataFrame.groupby(by='target').count().loc['negative',:][0]\n",
    "    neu_class_coutn = DataFrame.groupby(by='target').count().loc['neutral',:][0]\n",
    "    neu_nums = random.choices(range(neu_rows), k=pos_class_count-neu_class_coutn)\n",
    "    neg_nums = random.choices(range(neg_rows), k=pos_class_count-neg_class_count)\n",
    "    DataFrame = DataFrame.append(neutral_tweets.iloc[neu_nums,:], ignore_index=True)\n",
    "    DataFrame = DataFrame.append(negative_tweets.iloc[neg_nums,:], ignore_index=True)\n",
    "    return DataFrame\n",
    "\n",
    "def tweettag(tweet):\n",
    "    tags = [\n",
    "            'JJ',\n",
    "            'NN',\n",
    "            'NNS',\n",
    "            'IN',\n",
    "            'JJR',\n",
    "            'JJS',\n",
    "            'POS',\n",
    "            'RB',\n",
    "            'RBR',\n",
    "            'RBS',\n",
    "            'VB',\n",
    "            'VBD',\n",
    "            'BVG',\n",
    "            'VBN',\n",
    "            'VBP',\n",
    "            'VBZ',\n",
    "            'WRB'\n",
    "            ]\n",
    "    twt = []    \n",
    "    for tag in tags:\n",
    "        for word in pos_tag(my_extensible_tokenizer.findall(tweet), tagset='universial'):\n",
    "            if word[1] in tag:\n",
    "                twt.append((tag,True))\n",
    "    return(twt)\n",
    "\n",
    "def url_processsing(text, list):\n",
    "    url_matches = url_tokenizer.findall(text)\n",
    "    if url_matches:\n",
    "        list.append(('url_flag', True))\n",
    "#        list.append(('url_count', len(url_matches)))\n",
    "#        list.append(('url_count_2', len(url_matches)**3))\n",
    "#    elif not url_matches:\n",
    "#        list.append(('url_count', 0))\n",
    "#        list.append(('url_count_2', 0))\n",
    "    return list\n",
    "\n",
    "def hashtag_processing(text, list):\n",
    "    hashtag_matches = hashtag_tokenizer.findall(text)\n",
    "    if hashtag_matches:\n",
    "        list.append(('hashtag_flag', True))\n",
    "#        list.append(('hashtag_flag', len(hashtag_matches)))\n",
    "#        list.append(('hashtag_flag_2', len(hashtag_matches)**2))\n",
    "#    elif not hashtag_matches:\n",
    "#        list.append(('hashtag_flag', 0))\n",
    "#        list.append(('hashtag_flag_2', 0))\n",
    "    return list\n",
    "\n",
    "def textPolarity(str, list, sia):\n",
    "    pol = sia.polarity_scores(str)\n",
    "    if pol['neg'] > .8:\n",
    "        list.append((str + '(neg)', True))\n",
    "    if pol['pos'] > .8:\n",
    "        list.append((str + '(pos)', True))\n",
    "    if pol['neu'] > .8:\n",
    "        list.append((str + '(neu)', True))\n",
    "    return list\n",
    "\n",
    "def preprocessing(data):\n",
    "    tokens = [] # stores all features for dataset.  \n",
    "    sia = nltk.sentiment.vader.SentimentIntensityAnalyzer()\n",
    "    for text in data.values: # for each tweet.  \n",
    "        temp = []  # stores the modified matches for a single tweet\n",
    "        temp3 = [] # stores just the words (with negation, if applicable) for inclusion in n-grams\n",
    "        NOT = 0 # setting variable to 0 that will trigger the variable \"negate\" to append \"_not\" to all words following \"n't\" or \"not\" in a tweet\n",
    "        negate = 0 # set variable to 0 that, when equal to 1, will append \"_not\" to all words following an instance of \"n't\" or \"not\"\n",
    "        for matches in my_extensible_tokenizer.findall(text[1]):\n",
    "            matches = matches.lower()\n",
    "            # determine if matches is a url.\n",
    "            # Set NOT to 1, where it will remain for remainder of tweet until new row resets NOT to 0\n",
    "            if (matches[-3:] == \"n't\" or matches in negation_words):\n",
    "                NOT = 1\n",
    "            trash_matches = trash_tokenizer.findall(matches)\n",
    "            temp = url_processsing(matches, temp)\n",
    "            temp = hashtag_processing(matches, temp)\n",
    "            temp = textPolarity(matches, temp, sia) \n",
    "            # if the match is unwanted, then won't add to temp.  \n",
    "            if not trash_matches and matches !='':\n",
    "                #whatisthis = nltk.tag.pos_tag([matches])[0][1]\n",
    "                if matches not in stop_words:\n",
    "                    # If the NOT trigger was previously activated in this tweet, add \"_not\" to this (and all subsequent) words\n",
    "                    if negate == 1:\n",
    "                        matches = matches + '_not'\n",
    "                    temp.append(('contains(' + matches + ')', True))\n",
    "                    # Add to list of tokens in this tweet\n",
    "                    temp.append(('contains(' + matches.lower() + ')', True))\n",
    "                    # Add backward-looking 2-gram, if possible\n",
    "                    if len(temp3) > 0:\n",
    "                        temp.append(('contains(' + temp3[-1] + ' ' + matches.lower() + ')', True))\n",
    "                    # Add backward-looking 3-gram, if possible\n",
    "                    if len(temp3) > 1:\n",
    "                        temp.append(('contains(' + temp3[-2] + ' ' + temp3[-1] + ' ' + matches.lower() + ')', True))\n",
    "                    # Add backward-looking 4-gram, if possible\n",
    "                    if len(temp3) > 2:\n",
    "                        temp.append(('contains(' + temp3[-3] + ' ' + temp3[-2] + ' ' + temp3[-1] + ' ' + matches.lower() + ')', True))\n",
    "                    # Add to running list of words being saved for n-grams\n",
    "                    temp3.append(matches.lower())\n",
    "            # Activate _not trigger if this word ended with \"n't\" or was \"not\"\n",
    "            if NOT == 1:\n",
    "                negate = 1\n",
    "        temp = temp + tweettag(text[1])\n",
    "        tokens.append((dict(temp), text[0]))\n",
    "    return tokens\n",
    "\n",
    "# Define function that performs VADER on dataset and returns accuracy of VADER's sentiment analysis\n",
    "def vader(data):\n",
    "    # Stores list of results of VADER process for each tweet\n",
    "    vader = []\n",
    "    # for each tweet  \n",
    "    for text in data.values:\n",
    "        # stores just the words (no negation) for inclusion in VADER\n",
    "        temp2 = []\n",
    "        # For each word in the tweet\n",
    "        for matches in my_extensible_tokenizer.findall(text[1]):\n",
    "            # If word doesn't register as a URL or username\n",
    "            trash_matches = trash_tokenizer.findall(matches)\n",
    "            if not trash_matches:\n",
    "                if matches != '':\n",
    "                    # And if word is not in list of stopwords\n",
    "                    if nltk.tag.pos_tag([matches])[0][1] not in stop_words:\n",
    "                        # Add to running list of tokens being saved for Vader\n",
    "                        temp2.append(matches.lower())\n",
    "        # Join all surviving tokens from tweet into clean single string\n",
    "        sentence = \" \".join(temp2)\n",
    "        # Run VADER on string  \n",
    "        vs = analyzer.polarity_scores(sentence)\n",
    "        # Assign score to tweet based on VADER's compound statistic\n",
    "        if vs['compound'] < -.05:\n",
    "            score = 'negative'\n",
    "        elif vs['compound'] > .05:\n",
    "            score = 'positive'\n",
    "        else:\n",
    "            score = 'neutral'\n",
    "        # Add this tweet's score to list of all scores\n",
    "        vader.append(score)\n",
    "    # Create dataframe that stores VADER sentiment result and true sentiment label in same row\n",
    "    vadertest = pd.DataFrame({'true':data['target'],'test':vader})\n",
    "    # Compute series that contains counts of each possible pairing of VADER result and true tweet sentiment\n",
    "    b = vadertest.groupby([\"true\", \"test\"]).size()\n",
    "\n",
    "    accuracy = (b['positive','positive']+b['neutral','neutral']+b['negative','negative'])/len(vadertest)\n",
    "    precision_pos = b['positive','positive']/(sum(vadertest['test']=='positive'))\n",
    "    recall_pos = b['positive','positive']/sum(vadertest['true']=='positive')\n",
    "    f_measure_pos = 2*(recall_pos * precision_pos) / (recall_pos + precision_pos)\n",
    "    precision_neu = b['neutral','neutral']/(sum(vadertest['test']=='neutral'))\n",
    "    recall_neu = b['neutral','neutral']/(sum(vadertest['true']=='neutral'))\n",
    "    f_measure_neu = 2*(recall_neu * precision_neu) / (recall_neu + precision_neu)\n",
    "    precision_neg = b['negative','negative']/(sum(vadertest['test']=='negative'))\n",
    "    recall_neg = b['negative','negative']/(sum(vadertest['true']=='negative'))\n",
    "    f_measure_neg = 2*(recall_neg * precision_neg) / (recall_neg + precision_neg)\n",
    "    recall_avg = (recall_pos + recall_neu + recall_neg)/3\n",
    "    \n",
    "    results = pd.Series({'Accuracy':accuracy,'Precision(Positive)':precision_pos,'Recall(Positive)':recall_pos,\n",
    "                     'F-Measure(Positive)':f_measure_pos,'Precision(Neutral)':precision_neu,'Recall(Neutral)':recall_neu,\n",
    "                     'F-Measure(Neutral)':f_measure_neu,'Precision(Negative)':precision_neg,'Precision(Neutral)':precision_neu,\n",
    "                     'F-Measure(Negative)':f_measure_neg,'Average Recall':recall_avg})\n",
    "    return(results)\n",
    "    \n",
    "data_dev = randomOversampler(data_dev)\n",
    "data_train = randomOversampler(data_train)\n",
    "data_devtest = randomOversampler(data_devtest)\n",
    "\n",
    "train_tokens = preprocessing(data_train)\n",
    "dev_tokens = preprocessing(data_dev)\n",
    "devtest_tokens = preprocessing(data_devtest)\n",
    "test_tokens = preprocessing(data_test)\n",
    "\n",
    "training_features = train_tokens + dev_tokens + devtest_tokens\n",
    "test_final = test_tokens # + devtest_tokens\n",
    "\n",
    "sentiment_analyzer = SentimentAnalyzer()\n",
    "trainer = NaiveBayesClassifier.train\n",
    "classifier = sentiment_analyzer.train(trainer=trainer, training_set=training_features)\n",
    "\n",
    "# Naive Bayes\n",
    "\n",
    "# Evaluating model on training data\n",
    "#sentiment_analyzer.evaluate(training_features, classifier)\n",
    "# Evaluating model on test data\n",
    "res1 = sentiment_analyzer.evaluate(test_final, classifier)\n",
    "res1\n",
    "\n",
    "# Vader \n",
    "\n",
    "# Evaluating model on training data\n",
    "#vader(data_train)\n",
    "# Evaluating model on test data\n",
    "res2 = vader(data_test)\n",
    "res2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Performance evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TBA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res11 = pd.DataFrame([res1])\n",
    "res22 = pd.DataFrame([res2.to_dict()])\n",
    "res = res11.append(res22)\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
